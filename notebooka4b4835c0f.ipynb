{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#WSU ID: y478x356\n#Name: Pranjal Nawarkar","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\n# Loading the train_data_insurance dataset\ntrain_data = pd.read_csv('/kaggle/input/cs770-assignment-2b/train_data_insurance.csv')\n\n\n# Exploring the training dataset\nprint(train_data.head())  # Displaying the first few rows\n","metadata":{"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"   Index  age     sex    bmi  children smoker     region      charges\n0      1   46  female  19.95         2     no  northwest   9193.83850\n1      2   47  female  24.32         0     no  northeast   8534.67180\n2      3   52  female  24.86         0     no  southeast  27117.99378\n3      4   39  female  34.32         5     no  southeast   8596.82780\n4      5   54  female  21.47         3     no  northwest  12475.35130\n","output_type":"stream"}]},{"cell_type":"code","source":"# Checking for missing values - fortunately there are no missing values to handle in the dataset\nmissing_values = train_data.isnull().sum()\nprint(\"Missing values:\\n\", missing_values)","metadata":{"execution":{"iopub.status.busy":"2023-10-25T16:50:05.480325Z","iopub.execute_input":"2023-10-25T16:50:05.481339Z","iopub.status.idle":"2023-10-25T16:50:05.490301Z","shell.execute_reply.started":"2023-10-25T16:50:05.481301Z","shell.execute_reply":"2023-10-25T16:50:05.489195Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Missing values:\n Index       0\nage         0\nsex         0\nbmi         0\nchildren    0\nsmoker      0\nregion      0\ncharges     0\ndtype: int64\n","output_type":"stream"}]},{"cell_type":"code","source":"# Identifyingg categorical columns\ncategorical_cols = train_data.select_dtypes(include=['object']).columns\nprint(\"Categorical columns:\", categorical_cols)\n\n# Using one-hot encoding to handle categoricall variables\ntrain_data = pd.get_dummies(train_data, columns=categorical_cols, drop_first=True)","metadata":{"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Categorical columns: Index(['sex', 'smoker', 'region'], dtype='object')\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Spliting the data into features variable (X) and target var (y)\nX = train_data.drop(columns=['charges'])\ny = train_data['charges']\n\n# Spliting the data into 'training' and 'validation sets'\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)","metadata":{"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\n# Creating a scaler and fit it to the training data\n#Here I am using StandardScalar to fit\nscaler = StandardScaler()\nscaler.fit(X_train)\n\n# Now, transforming the features using the scaler quant\nX_train_scaled = scaler.transform(X_train)\nX_val_scaled = scaler.transform(X_val)","metadata":{"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"from sklearn.svm import SVR\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n# Creating a Linear SVM model\nlinear_svm = SVR(kernel='linear')\n\n# Training this model on the scaled training data\nlinear_svm.fit(X_train_scaled, y_train)\n\n# Predicting on the validation set\ny_pred_linear = linear_svm.predict(X_val_scaled)\n\n# Evaluating the Linear SVM model\nmse_linear = mean_squared_error(y_val, y_pred_linear)\nr2_linear = r2_score(y_val, y_pred_linear)\n\nprint(\"Linear Kernel Model Evaluation:\")\nprint(\"Mean Squared Error:\", mse_linear)\nprint(\"R-squared:\", r2_linear)\n","metadata":{"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Linear Kernel Model Evaluation:\nMean Squared Error: 167685326.2079787\nR-squared: -0.051029194889683493\n","output_type":"stream"}]},{"cell_type":"code","source":"#Same mthod like before...\n# Creating a Polynomial SVM model\npoly_svm = SVR(kernel='poly')\n\n# Training the model on the scaled training data\npoly_svm.fit(X_train_scaled, y_train)\n\n# Predicting on the validation set\ny_pred_poly = poly_svm.predict(X_val_scaled)\n\n# Evaluating the Polynomial SVM model\nmse_poly = mean_squared_error(y_val, y_pred_poly)\nr2_poly = r2_score(y_val, y_pred_poly)\n\nprint(\"\\nPolynomial Kernel Model Evaluation:\")\nprint(\"Mean Squared Error:\", mse_poly)\nprint(\"R-squared:\", r2_poly)","metadata":{"execution":{"iopub.status.busy":"2023-10-25T16:50:18.245546Z","iopub.execute_input":"2023-10-25T16:50:18.245978Z","iopub.status.idle":"2023-10-25T16:50:18.297103Z","shell.execute_reply.started":"2023-10-25T16:50:18.245943Z","shell.execute_reply":"2023-10-25T16:50:18.295846Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"\nPolynomial Kernel Model Evaluation:\nMean Squared Error: 178421583.4423594\nR-squared: -0.11832261914067677\n","output_type":"stream"}]},{"cell_type":"code","source":"#Same like Linear + Poly...\n# Creating a Radial Kernel SVM model\nradial_svm = SVR(kernel='rbf')\n\n# Training the model on the scaled training data\nradial_svm.fit(X_train_scaled, y_train)\n\n# Predicting on the validation set\ny_pred_radial = radial_svm.predict(X_val_scaled)\n\n# Evaluating the Radial Kernel SVM model\nmse_radial = mean_squared_error(y_val, y_pred_radial)\nr2_radial = r2_score(y_val, y_pred_radial)\n\nprint(\"\\nRadial Kernel Model Evaluation:\")\nprint(\"Mean Squared Error:\", mse_radial)\nprint(\"R-squared:\", r2_radial)","metadata":{"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"\nRadial Kernel Model Evaluation:\nMean Squared Error: 178606797.83003694\nR-squared: -0.11948351814815039\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor\n\n#Now, that we have trained models using training data we will be-\n# Creating a Decision Tree model\ndecision_tree = DecisionTreeRegressor(random_state=42)\n\n# Training the model on the training data\ndecision_tree.fit(X_train, y_train)\n\n# Predicting on the validation set\ny_pred_tree = decision_tree.predict(X_val)\n\n# Evaluating the Decision Tree model\nmse_tree = mean_squared_error(y_val, y_pred_tree)\nr2_tree = r2_score(y_val, y_pred_tree)\n\nprint(\"\\nDecision Tree Model Evaluation:\")\nprint(\"Mean Squared Error:\", mse_tree)\nprint(\"R-squared:\", r2_tree)","metadata":{"execution":{"iopub.status.busy":"2023-10-25T16:50:23.868089Z","iopub.execute_input":"2023-10-25T16:50:23.868506Z","iopub.status.idle":"2023-10-25T16:50:23.908336Z","shell.execute_reply.started":"2023-10-25T16:50:23.868473Z","shell.execute_reply":"2023-10-25T16:50:23.907338Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"\nDecision Tree Model Evaluation:\nMean Squared Error: 55847028.82216392\nR-squared: 0.6499582935054276\n","output_type":"stream"}]},{"cell_type":"code","source":"\n#Our models are all set using training data\n\n#Now, we will deal with testing data set\n\n","metadata":{"execution":{"iopub.status.busy":"2023-10-25T16:50:25.977651Z","iopub.execute_input":"2023-10-25T16:50:25.978090Z","iopub.status.idle":"2023-10-25T16:50:25.983323Z","shell.execute_reply.started":"2023-10-25T16:50:25.978056Z","shell.execute_reply":"2023-10-25T16:50:25.982205Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\n# Loading the test dataset\ntest_data = pd.read_csv('/kaggle/input/cs770-assignment-2b/test_data_insurance.csv')\n\n# Checking for missing values\nmissing_values = test_data.isnull().sum()\nprint(\"Missing values:\\n\", missing_values)\n\n\n# Identifying categorical columns\ncategorical_cols = test_data.select_dtypes(include=['object']).columns\nprint(\"Categorical columns:\", categorical_cols)\n\n# Using one-hot encoding for categorical variables\ntest_data = pd.get_dummies(test_data, columns=categorical_cols, drop_first=True)\n\n\n# Now, I am scaling the features using the same scaler as in the training dataset\n# Creating a scaler and fit it to the training data\nscaler = StandardScaler()\nscaler.fit(X_train)\n\n# Transforming the test dataset features using the same scaler\nX_test_scaled = scaler.transform(test_data)\n\n# Now, X_test_scaled contains the preprocessed and scaled features of the test dataset, and we can use it for making predictions with your SVM models.\n","metadata":{"execution":{"iopub.status.busy":"2023-10-25T16:50:27.953957Z","iopub.execute_input":"2023-10-25T16:50:27.954999Z","iopub.status.idle":"2023-10-25T16:50:27.985941Z","shell.execute_reply.started":"2023-10-25T16:50:27.954966Z","shell.execute_reply":"2023-10-25T16:50:27.984827Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Missing values:\n Index       0\nage         0\nsex         0\nbmi         0\nchildren    0\nsmoker      0\nregion      0\ndtype: int64\nCategorical columns: Index(['sex', 'smoker', 'region'], dtype='object')\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.svm import SVR\n\n# Using the trained SVM models to predict on the test set\nX_test = test_data  # Already preprocessed and scaled\n\n# Predictions for our Linear SVM\ny_pred_linear_test = linear_svm.predict(X_test)\n\n# Predictions for our Polynomial SVM\ny_pred_poly_test = poly_svm.predict(X_test)\n\n# Predictions for ourr Radial SVM\ny_pred_radial_test = radial_svm.predict(X_test)\n\n# Creating a DataFrame for predictions\npredictions_df = pd.DataFrame({\n    'Index': test_data['Index'],  # Assuming 'Index' is the index column in the test data\n    'linear': y_pred_linear_test,\n    'polynomial': y_pred_poly_test,\n    'radial': y_pred_radial_test\n})\n\n# Saving the predictions to a CSV file\npredictions_df.to_csv('predictions_insurance.csv', index=False)\n","metadata":{"execution":{"iopub.status.busy":"2023-10-25T16:50:31.785659Z","iopub.execute_input":"2023-10-25T16:50:31.786696Z","iopub.status.idle":"2023-10-25T16:50:31.834109Z","shell.execute_reply.started":"2023-10-25T16:50:31.786658Z","shell.execute_reply":"2023-10-25T16:50:31.832874Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but SVR was fitted without feature names\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but SVR was fitted without feature names\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but SVR was fitted without feature names\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"\n# Explanation :--\n\n# In this section, I provide explanations, insights, and conclusions about the results.\n\n# Comparison of SVM Kernels:\n# I trained and evaluated three SVM models with different kernels: Linear, Polynomial, and Radial (RBF) as given in assignment.\n\n#Lower MSE values indicate better predictive performance.\n #R2 measures the proportion of the variance in the dependent variable (charges) that is explained by the independent variables (features). A higher R2 indicates a better fit.\n\n\n# My Performance Metrics:\n# - The Mean Squared Error (MSE) and R-squared (R2) were used to assess the performance of each model.\n# - The Linear Kernel model exhibited an MSE of 167,685,326.21 and an R2 of -0.05.\n# - The Polynomial Kernel model had an MSE of 178,421,583.44 and an R2 of -0.12.\n# - The Radial Kernel (RBF) model resulted in an MSE of 178,606,797.83 and an R2 of -0.12.\n\n# My Insights:\n# - Based on this comparison, I can conclude that the Linear Kernel is the best choice among the three SVM kernels.\n# - All three models exhibit limited predictive power with negative R-squared values, indicating poor fit to the data.\n# - The linear model performs slightly better in terms of MSE compared to the other two kernels.\n\n# Conclusion:\n# - In this analysis, none of the SVM kernels (Linear - slightly better) provide satisfactory results for insurance cost prediction.\n# - The negative R2 values suggest that linear and non-linear relationships are not adequately captured by these models.\n# - Further analysis and feature engineering may be required to improve predictive accuracy.\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}